{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 5 - Reinforcement Learning - MÁSTER EN DATA SCIENCE & AI : NUCLIO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ypTIoCpeiSW"
   },
   "source": [
    "# Actividad Reinforcemente Learning **Frozen lake problem**\n",
    "\n",
    "Resolver el problema del Frozen lake de OpenAI Gym. Documentación: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
    "\n",
    "## Objetivos\n",
    "- Conseguir movermos aleatoriamente hasta cumplir el objetivo\n",
    "- Conseguir que el agente aprenda con Q-learning\n",
    "- (Opcional) Probar con otros hiperparámetros\n",
    "- (Opcional) Modificar la recompensa\n",
    "\n",
    "## Consideraciones\n",
    "- No hay penalizaciones\n",
    "- Si el agente cae en un \"hole\", entonces done = True y se queda atascado sin poder salir (al igual que ocurre cuando llega al \"goal\")\n",
    "\n",
    "## Normas a seguir\n",
    "\n",
    "- Se debe entregar un **ÚNICO GOOGLE COLAB notebook** (archivo .ipynb) que incluya las instrucciones presentes y su **EJECUCIÓN!!!**.\n",
    "- Poner el nombre del grupo en el nombre del archivo y el nombre de todos los integrantes del grupo al inicio del notebook.\n",
    "\n",
    "## Criterio de evaluación\n",
    "\n",
    "- Seguimiento de las normas establecidas en la actividad.\n",
    "- Corrección en el uso de algoritmos, modelos y formas idiomáticas en Python.\n",
    "- El código debe poder ejecutarse sin modificación alguna en Google Colaboratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ly604VTn1ue"
   },
   "source": [
    "## **Instalamos librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3046,
     "status": "ok",
     "timestamp": 1730634126492,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "ttylT1EzLVUY",
    "outputId": "a9555a6c-10cd-4e93-b195-380866fed41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730634126492,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "-aiKby2RNy-T"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shd3NqyQn9IO"
   },
   "source": [
    "## **Definición del entorno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730634126492,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "1_3z3ZByoAcO"
   },
   "outputs": [],
   "source": [
    "# Definimos el entorno\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730634126492,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "E_Nw22y00NEH",
    "outputId": "0097d2a9-c338-4414-b932-19b622bc6085"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Fijamos una semilla\n",
    "seed_value = 42\n",
    "env.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1415,
     "status": "ok",
     "timestamp": 1730634127902,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "fETKbHsBOGtB",
    "outputId": "e9708ade-d5a4-44af-b42d-2133c5de23b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "env.reset() # En este caso, empieza desde la misma posición inicial\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1730634127902,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "x0p9Zxwz0UNs",
    "outputId": "2a1e3bab-b96b-4417-d639-859d481099cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(4)\n",
      "State Space Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JarMsz_-0YfL"
   },
   "source": [
    "Acciones posibles:\n",
    "* 0: izquierda\n",
    "* 1: abajo\n",
    "* 2: derecha\n",
    "* 3: arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730634127902,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "Xyya51AU0shk",
    "outputId": "d3d789fb-8bda-455d-de3a-a9c6607a2607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0\n"
     ]
    }
   ],
   "source": [
    "# Identificador de estado\n",
    "state = env.s\n",
    "print(\"State:\", state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAHw0ZBm1C1-"
   },
   "source": [
    "## **¡Nos movemos aleatoriamente!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1730634128124,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "itxDrfce3-x0"
   },
   "outputs": [],
   "source": [
    "steps = 0\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1730634128364,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "D5EoVvD61Gjb",
    "outputId": "294878e9-d4dc-4865-c3ae-99cbb71eefba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 4\n",
      "4 0.0 False {'prob': 1.0}\n",
      "Step: 1\n"
     ]
    }
   ],
   "source": [
    "# Acciones: 0=izquierda, 1=abajo, 2=derecha, 3=arriba\n",
    "action = 1\n",
    "state, reward, done, info = env.step(action)\n",
    "\n",
    "print(\"State:\", state)\n",
    "print(state, reward, done, info)\n",
    "\n",
    "env.s = state\n",
    "env.render()\n",
    "\n",
    "steps += 1\n",
    "\n",
    "print(f\"Step: {steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GckRJZnXlFjB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YsT0TKTLVUc"
   },
   "source": [
    "## **Resolución del problema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1730634128364,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "27RQd1D7LVUc"
   },
   "outputs": [],
   "source": [
    "# Parámetros del algoritmo Q-Learning\n",
    "alpha = 0.1         # Tasa de aprendizaje\n",
    "gamma = 0.99        # Factor de descuento\n",
    "epsilon = 1.0       # Valor inicial de epsilon para exploración\n",
    "epsilon_min = 0.01  # Mínimo valor de epsilon\n",
    "epsilon_decay = 0.999 # Tasa de decremento de epsilon\n",
    "episodes = 20000     # Número de episodios para entrenamiento\n",
    "max_steps = 100     # Máximo número de pasos por episodio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730634128365,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "r__MYCqlLVUc"
   },
   "outputs": [],
   "source": [
    "# Inicializamos la tabla Q con ceros\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730634128365,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "zTN0tpxpLVUc"
   },
   "outputs": [],
   "source": [
    "# Función para seleccionar una acción usando la política epsilon-greedy\n",
    "def choose_action(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Exploración: selecciona una acción aleatoria\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Explotación: selecciona la mejor acción según la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10413,
     "status": "ok",
     "timestamp": 1730634138775,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "al2XlhxGLVUc",
    "outputId": "9d152639-e0e4-4cf7-a943-dfe32907f5ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla Q final:\n",
      "[[0.94148015 0.93206535 0.95099005 0.94148015]\n",
      " [0.94148015 0.         0.96059601 0.95099005]\n",
      " [0.95099005 0.970299   0.95099005 0.96059601]\n",
      " [0.96059601 0.         0.90255331 0.86023071]\n",
      " [0.89430452 0.73695422 0.         0.94148015]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.         0.96059601]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.23436875 0.         0.92890457 0.62118314]\n",
      " [0.68788948 0.67713159 0.9801     0.        ]\n",
      " [0.97029895 0.99       0.         0.970299  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.50480754 0.9899999  0.5589465 ]\n",
      " [0.98009861 0.99       1.         0.9801    ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del agente\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()  # Reinicia el entorno y obtiene el estado inicial\n",
    "    done = False\n",
    "    step = 0\n",
    "    global epsilon\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = choose_action(state)\n",
    "\n",
    "        # Tomamos la acción en el entorno\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Actualizamos la tabla Q usando la ecuación de Q-Learning\n",
    "        q_value = q_table[state, action]\n",
    "        max_q_value_next = np.max(q_table[next_state])  # Mejor estimación futura\n",
    "        q_table[state, action] = q_value + alpha * (reward + gamma * max_q_value_next - q_value)\n",
    "\n",
    "        # Actualizamos el estado\n",
    "        state = next_state\n",
    "\n",
    "        # Si llegamos al objetivo o caemos en un hueco, terminamos el episodio\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Reducimos epsilon (para disminuir la exploración)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# Mostrar la tabla Q entrenada\n",
    "print(\"Tabla Q final:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1611,
     "status": "ok",
     "timestamp": 1730634140381,
     "user": {
      "displayName": "Alejandro",
      "userId": "07391299381036780395"
     },
     "user_tz": -60
    },
    "id": "hvVH8DYXLVUc",
    "outputId": "840d3bf0-bf73-430f-bc4e-03d1a8c96613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prueba del agente entrenado ---\n",
      "Step: 0, State: 0, Action: 2, Reward: 0.0, Done: False\n",
      "Step: 1, State: 1, Action: 2, Reward: 0.0, Done: False\n",
      "Step: 2, State: 2, Action: 1, Reward: 0.0, Done: False\n",
      "Step: 3, State: 6, Action: 1, Reward: 0.0, Done: False\n",
      "Step: 4, State: 10, Action: 1, Reward: 0.0, Done: False\n",
      "Step: 5, State: 14, Action: 2, Reward: 1.0, Done: True\n",
      "Prueba conseguida\n"
     ]
    }
   ],
   "source": [
    "# Probar el agente\n",
    "state = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "print(\"\\n--- Prueba del agente entrenado ---\")\n",
    "while not done:\n",
    "    # Elegimos la acción óptima (sin exploración)\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    # Mostramos el entorno\n",
    "    env.render()\n",
    "    print(f\"Step: {steps}, State: {state}, Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "    # Actualizamos el estado\n",
    "    state = next_state\n",
    "    steps += 1\n",
    "\n",
    "    # Si el agente cae en un hueco o alcanza el objetivo, termina la prueba\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Imprimimos el resultado según el último estado\n",
    "if reward == 1:\n",
    "    print(\"Prueba conseguida\")\n",
    "else:\n",
    "    print(\"Te caíste\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
